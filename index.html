<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>

  <title>Zheng Lian</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="Zheng Lian is currently a assistant researcher at National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA)">
  <meta name="keywords" content="Zheng Lian, 连政, Deep Learning, Affective Computing">
  <meta name="author" content="Zheng Lian" />
  <link rel="stylesheet" href="w3.css">

  <style>
  .w3-sidebar a {font-family: "Roboto", sans-serif}
  body,h1,h2,h3,h4,h5,h6,.w3-wide {font-family: "Montserrat", sans-serif;}
  </style>

  <link rel="icon" type="image/png" href="images/icons.png">
  <!--
  <script src="jquery.min.js"></script>
  <script>
  $(document).ready(function(){
    // Add smooth scrolling to all links
    $("a").on('click', function(event) {

      // Make sure this.hash has a value before overriding default behavior
      if (this.hash !== "") {
        // Prevent default anchor click behavior
        event.preventDefault();

        // Store hash
        var hash = this.hash;

        // Using jQuery's animate() method to add smooth page scroll
        // The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
        $('html, body').animate({
          scrollTop: $(hash).offset().top
        }, 800, function(){

          // Add hash (#) to URL when done scrolling (default click behavior)
          window.location.hash = hash;
        });
      } // End if
    });
  });
  </script>
  //-->

</head>


<body class="w3-content" style="max-width:1000px">

<!-- Sidebar/menu -->
<nav class="w3-sidebar w3-bar-block w3-black w3-collapse w3-top w3-right" style="z-index:3;width:150px" id="mySidebar">
  <div class="w3-container w3-display-container w3-padding-16">
    <h3><b>Zheng Lian</b></h3>
  </div>
  <div class="w3-padding-64 w3-text-light-grey w3-large" style="font-weight:bold">
    <a href="#home" class="w3-bar-item w3-button">Home</a>
	<a href="#news" class="w3-bar-item w3-button">News</a>
    <a href="#publications" class="w3-bar-item w3-button">Research</a>
    <a href="#award" class="w3-bar-item w3-button">Awards</a>
	<a href="#professionals" class="w3-bar-item w3-button">Academic Services</a>
	<a href="#projects" class="w3-bar-item w3-button">Projects</a>
	<a href="#patents" class="w3-bar-item w3-button">Patents</a>
  </div>
</nav>

<!-- Top menu on small screens -->
<header class="w3-bar w3-top w3-hide-large w3-black w3-xlarge">
  <div class="w3-bar-item w3-padding-24">Zheng Lian</div>
  <a href="javascript:void(0)" class="w3-bar-item w3-button w3-padding-24 w3-right"  style="font-stretch: extra-expanded;" onclick="w3_open()"><b>≡</b></a>
  </div>
</header>

<!-- Overlay effect when opening sidebar on small screens -->
<div class="w3-overlay w3-hide-large" onclick="w3_close()" style="cursor:pointer" title="close side menu" id="myOverlay"></div>

<!-- !PAGE CONTENT! -->
<div class="w3-main" style="margin-left:150px">

  <!-- Push down content on small screens -->
  <div class="w3-hide-large" style="margin-top:83px"></div>

<!-- The Home Section -->
    <div class="w3-container w3-center w3-padding-32" id="home">
      <img style="width: 80%;max-width: 200px" alt="profile photo" src="images/ZhengLian.jpg">
      <h1>Zheng Lian</h1>
        <p class="w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:600px">
          I am an Associate Professor at the Institute of Automation, Chinese Academy of Sciences. 
          My research interest primarily centers on human-centric AI and affective computing. 
          <strong>My short-term goal</strong> is to establish a new pathway toward more reliable and accurate emotion recognition techniques. 
          Please refer to our proposed <a style="color: #447ec9" href="https://arxiv.org/abs/2306.15401">EMER</a>, <a style="color: #447ec9" href="https://arxiv.org/abs/2407.07653">AffectGPT</a>, <a style="color: #447ec9" href="https://arxiv.org/pdf/2410.01495">OV-MER</a> for more details.
          <strong>My long-term goal</strong> is to enhance real-world human-AI interaction,  ensuring robustness, trust, privacy, and efficiency for responsible deployment.
		</p>
        <p class="w3-center">
          <a href="lianzheng2016@ia.ac.cn">Email</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?user=S34nWz0AAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
		      <a href="https://github.com/zeroQiaoba">Github</a> &nbsp/&nbsp
          <a href="https://drive.google.com/file/d/14AVnZpVkLp82OJJoChF58viN05NPtFa4/view?usp=sharing">CV</a> 
        </p>
        </tbody></table>
  </div>


<!-- The News Section -->
  <div class="w3-container w3-padding-32" id="news">
    <h2>News</h2>
  <b><p style="color: red;">Job Openings: I am recruiting visiting students in Institute of Automation, Chinese Academy of Sciences.</p></b>
  <p><li> [Oct, 2024] We extend the EMER task to Open-vocabulary MER <a style="color: #447ec9" href="https://arxiv.org/pdf/2410.01495">[link]</a> </p>
  <p><li> [Sep, 2024] <a style="color: #447ec9" href="https://arxiv.org/pdf/2406.11161">Emotion-LLaMA</a> is accepted by NeurIPS2024 </p>
  <p><li> [Aug, 2024] We organize MEIJU'25 Challenge@ICASSP <a style="color: #447ec9" href="https://ai-s2-lab.github.io/MEIJU2025-website/">[link]</a> </p>
	<p><li> [Jul, 2024] <a style="color: #447ec9" href="https://arxiv.org/abs/2401.00416">SVFAP</a> is accepted by TAC </p>
	<p><li> [Jul, 2024] We update AffectGPT and build EMER-Corase dataset to promote the development of EMER <a style="color: #447ec9" href="https://arxiv.org/pdf/2407.07653">[link]</a> </p>
  <p><li> [Apr, 2024] We organize MER'24 Challenge@IJCAI and MRAC'24 Workshop@ACM Multimedia <a style="color: #447ec9" href="https://zeroqiaoba.github.io/MER2024-website/">[link]</a> </p>
	<p><li> [Mar, 2024] <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/pii/S156625352400160X">HiCMAE </a> is accepted by Information Fusion </a> </p>
	<p><li> [Mar, 2024] <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/pii/S1566253524001453">GPT-4V with Emotion </a> is accepted by Information Fusion </a> </p>
	<p><li> [Jan, 2024] We build <a style="color: #447ec9" href="https://arxiv.org/pdf/2401.03429.pdf">MERBench</a>, a unified evaluation benchmark for multimodal emotion recognition</p>
	<p><li> [Dec, 2023] We evaluate GPT-4V on 6 tasks and 21 datasets for multimodal emotion understanding <a style="color: #447ec9" href="https://arxiv.org/abs/2312.04293v1">[link]</a></p>
	<p><li> [Nov, 2023] Two papers are accepted by NeurIPS </p>
	<p><li> [Oct, 2023] <a style="color: #447ec9" href="https://dl.acm.org/doi/pdf/10.1145/3581783.3612365">MAE-DFER</a> is accepted by ACM MM </p>
	<p><li> [Aug, 2023] We propose <a style="color: #447ec9" href="https://arxiv.org/abs/2306.15401v3">EMER and AffectGPT</a> for more reliable affective computing techniques </p>
	<p><li> [Apr, 2023] We organize MER'23 Challenge and MRAC'23 Workshop@ACM Multimedia <a style="color: #447ec9" href="http://merchallenge.cn/">[link]</a></p>
	<p><li> [May, 2023] <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10122560"> EMT-DLFR </a> is accepted by TAC </p>
	<p><li> [Jan, 2023] <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10008078">GCNet</a> is accepted by TPAMI </p>
	<p><li> [Jul, 2022] <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9839579">PIRNet</a> is accepted by TNNLS </p>
	<p><li> [Jan, 2022] <a style="color: #447ec9" href="https://ieeexplore.ieee.org/document/9674867">SMIN</a> is accepted by TAC </p>
	<p><li> [Jan, 2021] <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9316758">CTNet</a> is accepted by TASLP </p>
  </div>  
  
  
 <!-- The Publications Section -->
  <div class="w3-container w3-padding-32"" id="publications">
    <h2> Research </h2>
	(* Equal contribution; † Corresponding author)
    <ol>
	  
    <p>
        <li><strong>Open-vocabulary Multimodal Emotion Recognition: Dataset, Metric, and Benchmark</strong>
        <br>
      <strong>Zheng Lian</strong>, Haiyang Sun, Licai Sun, Lan Chen, Haoyu Chen, Hao Gu, Zhuofan Wen, Shun Chen, Siyuan Zhang, Hailiang Yao, Mingyu Xu, Kang Chen, Bin Liu, Rui Liu, Shan Liang, Ya Li, Jiangyan Yi, Jianhua Tao
        <br>
      <em> Arxiv </em> 2024 | <a style="color: #447ec9" href="https://arxiv.org/pdf/2410.01495">paper</a>
    </p>

	  <p>
      <li><strong>AffectGPT: Dataset and Framework for Explainable Multimodal Emotion Recognition</strong>
      <br>
	  <strong>Zheng Lian</strong>, Haiyang Sun, Licai Sun, Jiangyan Yi, Bin Liu, Jianhua Tao
      <br>
	  <em> Arxiv </em> 2024 | <a style="color: #447ec9" href="https://arxiv.org/abs/2407.07653">paper</a>
    </p>

	  <p>
      <li><strong>Explainable Multimodal Emotion Recognition</strong>
      <br>
	  <strong>Zheng Lian</strong>, Haiyang Sun, Licai Sun, Hao Gu, Zhuofan Wen, Siyuan Zhang, etc.
      <br>
	  <em> Arxiv </em> 2024 | <a style="color: #447ec9" href="https://arxiv.org/abs/2306.15401">paper</a>
      </p>
	  
	  <p>
      <li><strong>MER 2024: Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition</strong>
      <br>
	  <strong>Zheng Lian</strong>, Haiyang Sun, Licai Sun, Zhuofan Wen, Siyuan Zhang, Shun Chen, Hao Gu, etc.
      <br>
	  <em> Arxiv </em> 2024 | <a style="color: #447ec9" href="https://arxiv.org/abs/2404.17113">paper</a>
      </p>
	  
	  <p>
      <li><strong>GPT-4V with Emotion: A Zero-shot Benchmark for Generalized Emotion Recognition</strong>
      <br>
	  <strong>Zheng Lian</strong>, Licai Sun, Haiyang Sun, Kang Chen, Zhuofan Wen, Hao Gu, Bin Liu, Jianhua Tao
      <br>
	  <em> Information Fusion </em> 2024 | <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/pii/S1566253524001453">paper</a>
      </p>
    

      <p>
        <li><strong>Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning</strong>
        <br>
        Zebang Cheng, Zhi-Qi Cheng, Jun-Yan He, Jingdong Sun, Kai Wang, Yuxiang Lin, <strong>Zheng Lian</strong>, Xiaojiang Peng, Alexander Hauptmann
        <br>
      <em> NeurIPS </em> 2024 | <a style="color: #447ec9" href="https://arxiv.org/abs/2406.11161">paper</a>
        </p>
	  
	  <p>
      <li><strong>HiCMAE: Hierarchical Contrastive Masked Autoencoder for Self-Supervised Audio-Visual Emotion Recognition</strong>
      <br>
	  Licai Sun, <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao
      <br>
	  <em> Information Fusion </em> 2024 | <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/pii/S156625352400160X">paper</a>
      </p>
	  
	  <p>
      <li><strong>SVFAP: Self-supervised Video Facial Affect Perceiver</strong>
      <br>
	  Licai Sun, <strong>Zheng Lian</strong>, Kexin Wang, Yu He, Mingyu Xu, Haiyang Sun, Bin Liu, Jianhua Tao
      <br>
	  <em> IEEE Transactions on Affective Computing (TAC) </em> 2024 | <a style="color: #447ec9" href="https://arxiv.org/abs/2401.00416">paper</a>
      </p>
	  
	  <p>
      <li><strong>Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning</strong>
      <br>
	  Kang Chen*, <strong>Zheng Lian*</strong>, Haiyang Sun, Bin Liu, Jianhua Tao
      <br>
	  <em> Arxiv </em> 2024 | <a style="color: #447ec9" href="https://arxiv.org/pdf/2402.11432.pdf">paper</a>
      </p>
	  
	  
	  <p>
      <li><strong>Pseudo Labels Regularization for Imbalanced Partial-Label Learning</strong>
      <br>
	  Mingyu Xu, <strong>Zheng Lian</strong>, Bin Liu, Zerui Chen, Jianhua Tao
      <br>
      <em> ICASSP </em> 2024 |  <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10448034">paper</a>
      </p>
	  
	  <p>
      <li><strong>NLoPT: N-gram Enhanced Low-Rank Task Adaptive Pre-training for Efficient Language Model Adaption</strong>
      <br>
	  Hao Gu, Jiangyan Yi, <strong>Zheng Lian</strong>, Jianhua Tao, Xinrui Yan
      <br>
	  <em> LREC-COLING </em> 2024
      </p>
	  
	  <p>
      <li><strong>Contrastive Learning based Modality-Invariant Feature Acquisition for Robust Multimodal Emotion Recognition with Missing Modalities</strong>
      <br>
	  Rui Liu, Haolin Zuo, <strong>Zheng Lian</strong>, Björn W. Schuller, Haizhou Li
      <br>
	  <em> IEEE Transactions on Affective Computing (TAC) </em> 2024 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10474146">paper</a>
      </p>

	  <p>
      <li><strong>MERBench: A Unified Evaluation Benchmark for Multimodal Emotion Recognition</strong>
      <br>
	  <strong>Zheng Lian</strong>, Licai Sun, Yong Ren, Hao Gu, Haiyang Sun, Lan Chen, Bin Liu, Jianhua Tao
      <br>
	  <em> Arxiv </em> 2023 | <a style="color: #447ec9" href="https://arxiv.org/pdf/2401.03429.pdf">paper</a>
      </p>
	  
	  
	  
	  <p>
      <li><strong>GCNet: Graph Completion Network for Incomplete Multimodal Learning in Conversation</strong>
      <br>
	  <strong>Zheng Lian</strong>, Lan Chen, Licai Sun, Bin Liu, Jianhua Tao
      <br>
      <em> Transactions on Pattern Analysis and Machine Intelligence (TPAMI) </em> 2023 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10008078">paper</a>
      </p>
	  
	  <p>
      <li><strong>MER 2023: Multi-label Learning, Modality Robustness, and Semi-Supervised Learning</strong>
      <br>
	  <strong>Zheng Lian</strong>, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, etc.
      <br>
      <em> ACM Multimedia (Organize Grand Challenge) </em> 2023 |  <a style="color: #447ec9" href="https://dl.acm.org/doi/pdf/10.1145/3581783.3612836">paper</a>
      </p>
	  
	  
	  <p>
      <li><strong>ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning</strong>
      <br>
	  Mingyu Xu*, <strong>Zheng Lian*</strong>, Lei Feng, Bin Liu, Jianhua Tao
      <br>
      <em> NeurIPS </em> 2023 |  <a style="color: #447ec9" href="https://openreview.net/forum?id=PYSfn5xXEe">paper</a>
      </p>
	  
	  <p>
      <li><strong>VRA: Variational Rectified Activation for Out-of-distribution Detection</strong>
      <br>
	  Mingyu Xu, <strong>Zheng Lian†</strong>, Bin Liu, Jianhua Tao
      <br>
      <em> NeurIPS </em> 2023 |  <a style="color: #447ec9" href="https://openreview.net/forum?id=d0VItRE2ZH">paper</a>
      </p>
	  
	  <p>
      <li><strong>MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic Facial Expression Recognition</strong>
      <br>
	  Licai Sun, <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao
      <br>
      <em> ACM Multimedia </em> 2023 |  <a style="color: #447ec9" href="https://dl.acm.org/doi/pdf/10.1145/3581783.3612365">paper</a>
      </p>
	  
	  <p>
      <li><strong>Efficient multimodal transformer with dual-level feature restoration for robust multimodal sentiment analysis</strong>
      <br>
	  Licai Sun, <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao
      <br>
      <em> IEEE Transactions on Affective Computing (TAC) </em> 2023 |  <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10122560">paper</a>
      </p>

	  <p>
      <li><strong>EmotionNAS: Two-stream Architecture Search for Speech Emotion Recognition</strong>
      <br>
      Haiyang Sun*, <strong>Zheng Lian*</strong>, Bin Liu, Ying Li, Licai Sun, Cong Cai, Jianhua Tao, Meng Wang, Yuan Cheng
      <br>
      <em>Interspeech </em> 2023 | <a style="color: #447ec9" href="https://arxiv.org/pdf/2203.13617.pdf">paper</a>
      </p>
	  
	  <p>
      <li><strong>PIRNet: Personality-enhanced Iterative Refinement Network for Emotion Recognition in Conversation</strong>
      <br>
	  <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao
      <br>
      <em> IEEE Transactions on Neural Networks and Learning Systems (TNNLS) </em> 2022 |  <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9839579">Early Access</a>
      </p>
	  
	  <p>
      <li><strong>SMIN: Semi-supervised Multi-modal Interaction Network for Conversational Emotion Recognition</strong>
      <br>
	  <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao
      <br>
      <em> IEEE Transactions on Affective Computing (TAC) </em> 2022 |  <a style="color: #447ec9" href="https://ieeexplore.ieee.org/document/9674867">paper</a>
      </p>
		
	  <p>
      <li><strong>DECN: Dialogical Emotion Correction Network for Conversational Emotion Recognition</strong>
      <br>
      <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao 
      <br>
      <em>Neurocomputing</em> 2021 | <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/pii/S0925231221007542">paper</a>
      </p>
	  
	  <p>
      <li><strong>CTNet: Conversational Transformer Network for Emotion Recognition</strong>
      <br>
      <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao 
      <br>
      <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)</em> 2021 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9316758">paper</a>
      </p>
	  
	  <p>
      <li><strong>Towards Fine-Grained Prosody Control for Voice Conversion</strong>
      <br>
      <strong>Zheng Lian</strong>, Rongxiu Zhong, Zhengqi Wen, Bin Liu, Jianhua Tao
      <br>
      <em>Proceedings of the 12th International Symposium on Chinese Spoken Language Processing (ISCSLP) </em> 2021 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9362110">paper</a>
      </p>
	  
	  
	  <p>
      <li><strong>Multimodal Cross-and Self-Attention Network for Speech Emotion Recognition</strong>
      <br>
	  Licai Sun, Bin Liu, Jianhua Tao, <strong>Zheng Lian</strong>
      <br>
      <em> ICASSP</em> 2021 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9414654">paper</a>
      </p>
	  
	  <p>
      <li><strong>Conversational Emotion Recognition Using Self-Attention Mechanisms and Graph Neural Networks</strong>
      <br>
      <strong>Zheng Lian</strong>, Jianhua Tao, Bin Liu, Jian Huang, Zhanlei Yang, Rongjun Li
      <br>
      <em>Interspeech </em> 2020 | <a style="color: #447ec9" href="http://www.interspeech2020.org/uploadfile/pdf/Wed-1-9-6.pdf">paper</a>
      </p>
	  
	  <p>
      <li><strong>Context-Dependent Domain Adversarial Neural Network for Multimodal Emotion Recognition</strong>
      <br>
      <strong>Zheng Lian</strong>, Jianhua Tao, Bin Liu, Jian Huang, Zhanlei Yang, Rongjun Li
      <br>
      <em>Interspeech </em> 2020 | <a style="color: #447ec9" href="http://www.interspeech2020.org/uploadfile/pdf/Mon-1-9-9.pdf">paper</a>
      </p>
	
	  
	  <p>
      <li><strong>ARVC: An Auto-Regressive Voice Conversion System Without Parallel Training Data</strong>
      <br>
      <strong>Zheng Lian</strong>, Zhengqi Wen, Xinyong Zhou, Songbai Pu, Shengkai Zhang, Jianhua Tao
      <br>
      <em>Interspeech </em> 2020 | <a style="color: #447ec9" href="http://www.interspeech2020.org/uploadfile/pdf/Thu-3-4-7.pdf">paper</a>
      </p>	  
	  
	  <p>
      <li><strong>Learning Utterance-level Representations with Label Smoothing for Speech Emotion Recognition</strong>
      <br>
	  Jian Huang, Jianhua Tao, Bin Liu, <strong>Zheng Lian</strong>
      <br>
      <em>Interspeech </em> 2020 | <a style="color: #447ec9" href="http://www.interspeech2020.org/uploadfile/pdf/Thu-2-2-1.pdf">paper</a>
      </p>	
	  
	  <p>
      <li><strong>Multimodal Transformer Fusion for Continuous Emotion Recognition</strong>
      <br>
	  Jian Huang, Jianhua Tao, Bin Liu, <strong>Zheng Lian</strong>, Mingyue Niu
      <br>
      <em>ICASSP </em> 2020 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9053762">paper</a>
      </p>	
	  
	  <p>
      <li><strong>Multimodal Spatiotemporal Representation for Automatic Depression Level Detection</strong>
      <br>
	  Mingyue Niu, Jianhua Tao, Bin Liu, Jian Huang, <strong>Zheng Lian</strong>
      <br>
      <em>IEEE Transactions on Affective Computing (TAC) </em> 2020 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9226102">paper</a>
      </p>	

	  <p>
      <li><strong>Unsupervised Representation Learning with Future Observation Prediction for Speech Emotion Recognition</strong>
      <br>
      <strong>Zheng Lian</strong>, Jianhua Tao, Bin Liu, Jian Huang
      <br>
      <em>Interspeech </em> 2019 | <a style="color: #447ec9" href="https://www.isca-speech.org/archive/pdfs/interspeech_2019/lian19b_interspeech.pdf">paper</a>
      </p>	  

	  <p>
      <li><strong>Conversational Emotion Analysis via Attention Mechanisms</strong>
      <br>
      <strong>Zheng Lian</strong>, Jianhua Tao, Bin Liu, Jian Huang
      <br>
      <em>Interspeech </em> 2019 | <a style="color: #447ec9" href="https://www.isca-archive.org/interspeech_2019/lian19_interspeech.pdf">paper</a>
      </p>	 
	  
	  
	  <p>
      <li><strong>Discriminative video representation with temporal order for micro-expression recognition</strong>
      <br>
	  Mingyue Niu, Jianhua Tao, Ya Li, Jian Huang, <strong>Zheng Lian</strong>
      <br>
      <em>ICASSP </em> 2019 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/document/8682295">paper</a>
      </p>	 
	  
	  <p>
      <li><strong>End-to-End Continuous Emotion Recognition from Video Using 3D ConvLSTM Networks</strong>
      <br>
	  Jian Huang, Ya Li, Jianhua Tao, <strong>Zheng Lian</strong>, Jiangyan Yi
      <br>
      <em>ICASSP </em> 2018 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/document/8461963">paper</a>
      </p>	
	  
	  <p>
      <li><strong>Speech Emotion Recognition from Variable-Length Inputs with Triplet Loss Function</strong>
      <br>
	  Jian Huang, Ya Li, Jianhua Tao, <strong>Zheng Lian</strong>
      <br>
      <em>Interspeech </em> 2018 | <a style="color: #447ec9" href="https://www.isca-archive.org/interspeech_2018/huang18b_interspeech.pdf">paper</a>
      </p>	
	  	
      </ol>
	  

    </p>
  </div>


	
  <!-- The Awards Section -->
  <div class="w3-container w3-padding-32" id="award">
    <h2>Awards</h2>
  <p><li> 2024, Winner in the MuSe Challenge@ACM MM</p>
	<p><li> 2023, Best Poster in the First CSIG Conference on Emotional Intelligence </p>
	<p><li> 2023, Winner in the MEGC Challenge@ACM MM</p>
	<p><li> 2023, Outstanding Reviewer of ICASSP</p>
	<p><li> 2022, Winner in the MuSe Challenge@ACM MM</p>
	<p><li> 2021, The First Prize for Technological Invention of Chinese Institute of Electronics</p>
	<!-- 中国电子学会技术发明一等奖 -->
	<p><li> 2021, Best Paper for National Conference on Man-Machine Speech Communication (NCMMSC)</p>
	<!-- 第十六届全国人机语音通讯学术会议最佳论文奖 -->
	<p><li> 2021, Winner in the MuSe Challenge@ACM MM</p>
	<p><li> 2021, Climbing First-class Scholarship of Institute of Automation, Chinese Academy of Sciences</p>
	<!-- 中国科学院自动化研究所攀登一等奖学金 -->
	<p><li> 2020, Winner in the MuSe Challenge@ACM MM</p>
	<p><li> 2020, Interspeech Travel Grand</p>
	<p><li> 2020, Winner in Voice Conversion Challenge@Interspeech</p>
    <p><li> 2019, 2nd Place in the AVEC Challenge@ACM MM</p>
    <p><li> 2018, 2nd Place in the AVEC Challenge@ACM MM</p>
	<!--<p><li> 2017, "Three Good Students" of University of Chinese Academy of Sciences</p>-->
  </div>  
  
  
  <!-- The Professionals Section -->
  <div class="w3-container w3-padding-32" id="professionals">
    <h2>Academic Services</h2>
  <p><li> Co-organize MEIJU'25 Challenge@ICASSP, 2025 </p>
  <p><li> Co-organize MER'24 Challenge@IJCAI and MRAC'24 Workshop@ACM MM, 2024 </p>
	<p><li> Co-organize MER'23 Challenge@ACM MM and MRAC'23 Workshop@ACM MM, 2023 </p>
	<p><li> Committee Member, Chinese Society of Image and Graphics, Emotional Computing and Understanding Committee（2021-） </p>
	<!-- 中国图象图形学学会情感计算与理解专委会委员 -->
	<p><li> Committee Member, Chinese Information Processing Society of China, Emotional Computing Committee（2021-） </p>
	<!-- 中国中文信息学会情感计算专委会委员 -->
	<p><li> Committee Member, Chinese Association for Artificial Intelligence, Emotional Intelligence Committee（2021-） </p>
	<!-- 中国人工智能学会情感智能专委会委员 -->
  <p><li> Executive Committee Member, Speech Dialogue and Auditory Professional Committee, China Computer Federation (CCF)（2024-） </p>
  <p><li> Session Chair: ISCSLP </p>
	<p><li> Conference Program Committee: NeurIPS, ICLR, AAAI, ACL ARR, ACM Multimedia, ICASSP, Interspeech, etc </p>
  <p><li> Journal Reviewer: IJCV, TAC, TNNLS, TASLP, TOMM, TCSVT, TALLIP, IEEE Signal Processing Magazine, Information Fusion, Pattern Recognition, etc </p>
  </div> 
  

  <!-- The Projects Section -->
  <div class="w3-container w3-padding-32"" id="projects">
    <h2> Projects </h2>
	
    <ol>
	
      <p><li>National Natural Science Foundation of China, Youth Science Fund Project, 2023/1~2025/12, Host ￥300,000</p>
	    <!-- <p><li><strong>国家自然科学基金，青年科学基金项目，面向交互场景的情感识别方法研究，62201572，2023/1/1~2025/12/31，30万元，在研，主持</strong></p>-->
      
      <p><li>Chinese Academy of Sciences, 2023/1~2023/12, Host ￥500,000 (Total ￥9,600,000)</p>
      <!-- <p><li><strong>中国科学院重点部署项目，c*作业人员心理状态评估技术</strong></p>-->
        
      <p><li>National Natural Science Foundation of China, 2023/1~2026/12, Host ￥60,000 (Total ￥560,000)</p>
	    <!-- <p><li><strong>国家自然科学基金面上项目，小样本下基于语音内容理解的青少年抑郁症早期识别方法研究</strong></p>-->

      <p><li>Outstanding Youth Fund of State Key Laboratory of Multimodal Artificial Intelligence Systems, 2024/12~2025/12, Host ￥100,000 </p>
      <!-- <p><li><strong>多模态人工智能系统全国重点实验室2024年优秀青年基金，大模型与知识驱动的鲁棒细微情感交互技术</strong></p>-->
      
    </p>
  </div>
  
  
  <!-- The Patents Section -->
  <div class="w3-container w3-padding-32"" id="patents">
    <h2> Patents </h2>
	
    <ol>
      
	  <p>
      <li><strong>Dialogue emotion correction method based on graph neural network (US Patent No. 12100418), 2024/09/24 </strong>  <a style="color: #447ec9" href="https://patentimages.storage.googleapis.com/42/0c/46/0f263026e451a7/US12100418.pdf">Link</a>
      <br>
	  Jianhua Tao, <strong>Zheng Lian</strong>, Bin Liu, Xuefei Liu
      <br>
      </p>
	  
	  <p>
      <li><strong>Automatic lie detection method and apparatus for interactive scenarios, device and medium (US Patent No. 11238289), 2022/02/01 </strong>  <a style="color: #447ec9" href="https://patentimages.storage.googleapis.com/5f/ef/96/54eda53532f316/US11238289.pdf">Link</a>
      <br>
	  Jianhua Tao, <strong>Zheng Lian</strong>, Bin Liu, Licai Sun
      <br>
      </p>
	  
	  <p>
      <li><strong>Multimodal dimensional emotion recognition method (US Patent No. 11281945), 2022/03/22 </strong>  <a style="color: #447ec9" href="https://patentimages.storage.googleapis.com/28/6b/b6/d5d3ebfbcbeea4/US11281945.pdf">Link</a>
      <br>
	  Jianhua Tao, Licai Sun, Bin Liu, <strong>Zheng Lian</strong>
      <br>
      </p>
	  
	  <p>
      <li><strong>Multi-modal lie detection method and apparatus, and device (US Patent No. 11244119), 2022/02/08 </strong>  <a style="color: #447ec9" href="https://patentimages.storage.googleapis.com/c2/a6/4f/26204db8af95c5/US11244119.pdf">Link</a>
      <br>
	  Jianhua Tao, Licai Sun, Bin Liu, <strong>Zheng Lian</strong>
      <br>
      </p>
	  
	  <p>
      <li><strong>Expression recognition method under natural scene (US Patent No. 11216652), 2022/01/04</strong>  <a style="color: #447ec9" href="https://patentimages.storage.googleapis.com/22/2c/bc/043136d205e9c5/US11216652.pdf">Link</a>
      <br>
	  Jianhua Tao, Mingyuan Xiao, Bin Liu, <strong>Zheng Lian</strong>
      <br>
      </p>
	  
    </p>
  </div>
  
  
  
  <div class="w3-light-grey w3-center w3-padding-24">
  <script type="text/javascript">
  var sc_project=12347113; 
  var sc_invisible=0; 
  var sc_security="21aca5d1"; 
  var sc_https=1; 
  var scJsHost = "https://";
  document.write("<sc"+"ript type='text/javascript' src='" + scJsHost+
  "statcounter.com/counter/counter.js'></"+"script>");
  <noscript>
    <div class="statcounter"><a title="Web Analytics Made Easy -
  StatCounter" href="https://statcounter.com/" target="_blank"><img
  class="statcounter" src="https://c.statcounter.com/12347113/0/21aca5d1/0/"
  alt="Web Analytics Made Easy - StatCounter"></a></div>
  </noscript>
  <!-- End of Statcounter Code -->

  </div>

  <!-- End page content -->
</div>

<script>
// Accordion 
function myAccFunc() {
  var x = document.getElementById("demoAcc");
  if (x.className.indexOf("w3-show") == -1) {
    x.className += " w3-show";
  } else {
    x.className = x.className.replace(" w3-show", "");
  }
}

// Click on the "Jeans" link on page load to open the accordion for demo purposes
document.getElementById("myBtn").click();


// Open and close sidebar
function w3_open() {
  document.getElementById("mySidebar").style.display = "block";
  document.getElementById("myOverlay").style.display = "block";
}
 
function w3_close() {
  document.getElementById("mySidebar").style.display = "none";
  document.getElementById("myOverlay").style.display = "none";
}
</script>

</body>
</html>

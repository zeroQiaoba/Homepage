<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>

  <title>Zheng Lian</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="Zheng Lian is currently a assistant researcher at National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA)">
  <meta name="keywords" content="Zheng Lian, 连政, Deep Learning, Affective Computing">
  <meta name="author" content="Zheng Lian" />
  <link rel="stylesheet" href="w3.css">

  <style>
  .w3-sidebar a {font-family: "Roboto", sans-serif}
  body,h1,h2,h3,h4,h5,h6,.w3-wide {font-family: "Montserrat", sans-serif;}
  </style>

  <link rel="icon" type="image/png" href="images/icons.png">
  <!--
  <script src="jquery.min.js"></script>
  <script>
  $(document).ready(function(){
    // Add smooth scrolling to all links
    $("a").on('click', function(event) {

      // Make sure this.hash has a value before overriding default behavior
      if (this.hash !== "") {
        // Prevent default anchor click behavior
        event.preventDefault();

        // Store hash
        var hash = this.hash;

        // Using jQuery's animate() method to add smooth page scroll
        // The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
        $('html, body').animate({
          scrollTop: $(hash).offset().top
        }, 800, function(){

          // Add hash (#) to URL when done scrolling (default click behavior)
          window.location.hash = hash;
        });
      } // End if
    });
  });
  </script>
  //-->

</head>


<body class="w3-content" style="max-width:1000px">

<!-- Sidebar/menu -->
<nav class="w3-sidebar w3-bar-block w3-black w3-collapse w3-top w3-right" style="z-index:3;width:150px" id="mySidebar">
  <div class="w3-container w3-display-container w3-padding-16">
    <h3><b>Zheng Lian</b></h3>
  </div>
  <div class="w3-padding-64 w3-text-light-grey w3-large" style="font-weight:bold">
    <a href="#home" class="w3-bar-item w3-button">Home</a>
	<a href="#news" class="w3-bar-item w3-button">News</a>
    <a href="#publications" class="w3-bar-item w3-button">Research</a>
    <a href="#award" class="w3-bar-item w3-button">Awards</a>
	<a href="#professionals" class="w3-bar-item w3-button">Academic Services</a>
	<a href="#projects" class="w3-bar-item w3-button">Projects</a>
	<a href="#patents" class="w3-bar-item w3-button">Patents</a>
  </div>
</nav>

<!-- Top menu on small screens -->
<header class="w3-bar w3-top w3-hide-large w3-black w3-xlarge">
  <div class="w3-bar-item w3-padding-24">Zheng Lian</div>
  <a href="javascript:void(0)" class="w3-bar-item w3-button w3-padding-24 w3-right"  style="font-stretch: extra-expanded;" onclick="w3_open()"><b>≡</b></a>
  </div>
</header>

<!-- Overlay effect when opening sidebar on small screens -->
<div class="w3-overlay w3-hide-large" onclick="w3_close()" style="cursor:pointer" title="close side menu" id="myOverlay"></div>

<!-- !PAGE CONTENT! -->
<div class="w3-main" style="margin-left:150px">

  <!-- Push down content on small screens -->
  <div class="w3-hide-large" style="margin-top:83px"></div>

<!-- The Home Section -->
    <div class="w3-container w3-center w3-padding-32" id="home">
      <img style="width: 80%;max-width: 200px" alt="profile photo" src="images/ZhengLian.jpg">
      <h1>Zheng Lian</h1>
        <p class="w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:600px">
          I am an Associate Professor at <a style="color: #447ec9" href="https://ia.cas.cn/rcdw/fyjy/202503/t20250317_7558714.html">State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences.</a>
          My research interest primarily centers on human-centric AI and affective computing. 
          In this field, I co-organize a series of challenges and workshops (<a style="color: #447ec9" href="http://merchallenge.cn/workshop">MER@IJCAI</a>, 
          <a style="color: #447ec9" href="https://zeroqiaoba.github.io/MER2024-website/">MRAC@ACM Multimedia</a>,
          <a style="color: #447ec9" href="https://ai-s2-lab.github.io/MEIJU2025-website">MEIJU@ICASSP</a>), 
          establish benchmark (<a style="color: #447ec9" href="https://arxiv.org/pdf/2401.03429">MERBench</a>) and 
          toolbox (<a style="color: #447ec9" href="https://github.com/zeroQiaoba/MERTools">MERTools</a>)
          , and propose new tasks to enhance accuracy and reliability (<a style="color: #447ec9" href="https://arxiv.org/abs/2306.15401">EMER</a>,
          <a style="color: #447ec9" href="https://arxiv.org/abs/2410.01495">OV-MER</a>).
          I also serve as Associate Editor at <a style="color: #447ec9" href="https://www.computer.org/csdl/journal/ta/about/107327?title=Editorial%20Board&periodical=IEEE%20Transactions%20on%20Affective%20Computing">IEEE Transactions on Affective Computing (TAC)</a>, 
          Area Chair at <a style="color: #447ec9" href="https://acmmm2025.org/">ACM Multimedia 2025</a>,
          Area Editor at <a style="color: #447ec9" href="https://www.sciencedirect.com/journal/information-fusion/about/editorial-board">Information Fusion</a>, 
          and Session Chair at <a style="color: #447ec9" href="http://www.iscslp2024.com/home">ISCSLP 2024</a>.
          My short-term goal is to establish a new pathway toward more reliable and accurate emotion recognition techniques. 
          My long-term goal is to enhance real-world human-AI interaction, ensuring robustness, trust, privacy, efficiency, and responsible deployment 
          in areas such as healthcare, education, finance, etc. 
		</p>
        <p class="w3-center">
          <a href="lianzheng2016@ia.ac.cn">Email</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?user=S34nWz0AAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
		      <a href="https://github.com/zeroQiaoba">Github</a> &nbsp/&nbsp
          <a href="https://drive.google.com/file/d/14AVnZpVkLp82OJJoChF58viN05NPtFa4/view?usp=sharing">CV</a> 
        </p>
        </tbody></table>
  </div>


<!-- The News Section -->
  <div class="w3-container w3-padding-32" id="news">
    <h2>News</h2>
  <b><p style="color: red;">Job Openings: I am recruiting visiting students in Institute of Automation, Chinese Academy of Sciences.</p></b>
  <p><li> [Apr, 2025] I will co-organize MER'25 Challenge and MRAC'25 Workshop@ACM Multimedia <a style="color: #447ec9" href="https://zeroqiaoba.github.io/MER2025-website/">[link]</a> </p>
  <p><li> [Mar, 2025] I will serve as Area Chair at <a style="color: #447ec9" href="https://acmmm2025.org/">ACM Multimedia 2025.</a></p>
  <p><li> [Mar, 2025] Two paper are accepted by ICASSP2025 </p>
  <p><li> [Feb, 2025] I will serve as Area Editor at <a style="color: #447ec9" href="https://www.sciencedirect.com/journal/information-fusion/about/editorial-board">Information Fusion.</a></p>
  <p><li> [Jan, 2025] I will serve as Associate Editor at <a style="color: #447ec9" href="https://www.computer.org/csdl/journal/ta/about/107327?title=Editorial%20Board&periodical=IEEE%20Transactions%20on%20Affective%20Computing">IEEE Transactions on Affective Computing (TAC).</a></p>
  <p><li> [Nov, 2024] I will serve as Session Chair at <a style="color: #447ec9" href="http://www.iscslp2024.com/home">ISCSLP 2024.</a></p>
  <p><li> [Oct, 2024] We extend the EMER task to Open-vocabulary MER <a style="color: #447ec9" href="https://arxiv.org/pdf/2410.01495">[link]</a> </p>
  <p><li> [Sep, 2024] <a style="color: #447ec9" href="https://arxiv.org/pdf/2406.11161">Emotion-LLaMA</a> is accepted by NeurIPS2024 </p>
  <p><li> [Aug, 2024] We organize MEIJU'25 Challenge@ICASSP <a style="color: #447ec9" href="https://ai-s2-lab.github.io/MEIJU2025-website/">[link]</a> </p>
	<p><li> [Jul, 2024] <a style="color: #447ec9" href="https://arxiv.org/abs/2401.00416">SVFAP</a> is accepted by TAC </p>
	<p><li> [Jul, 2024] We update AffectGPT and build EMER-Corase dataset to promote the development of EMER <a style="color: #447ec9" href="https://arxiv.org/pdf/2407.07653">[link]</a> </p>
  <p><li> [Apr, 2024] We organize MER'24 Challenge@IJCAI and MRAC'24 Workshop@ACM Multimedia <a style="color: #447ec9" href="https://zeroqiaoba.github.io/MER2024-website/">[link]</a> </p>
	<p><li> [Mar, 2024] <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/pii/S156625352400160X">HiCMAE </a> is accepted by Information Fusion </a> </p>
	<p><li> [Mar, 2024] <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/pii/S1566253524001453">GPT-4V with Emotion </a> is accepted by Information Fusion </a> </p>
	<p><li> [Jan, 2024] We build <a style="color: #447ec9" href="https://arxiv.org/pdf/2401.03429.pdf">MERBench</a>, a unified evaluation benchmark for multimodal emotion recognition</p>
	<p><li> [Nov, 2023] Two papers are accepted by NeurIPS </p>
	<p><li> [Oct, 2023] <a style="color: #447ec9" href="https://dl.acm.org/doi/pdf/10.1145/3581783.3612365">MAE-DFER</a> is accepted by ACM MM </p>
	<p><li> [Aug, 2023] We propose <a style="color: #447ec9" href="https://arxiv.org/abs/2306.15401v3">EMER and AffectGPT</a> for more reliable affective computing techniques </p>
	<p><li> [Apr, 2023] We organize MER'23 Challenge and MRAC'23 Workshop@ACM Multimedia <a style="color: #447ec9" href="http://merchallenge.cn/">[link]</a></p>
	<p><li> [May, 2023] <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10122560"> EMT-DLFR </a> is accepted by TAC </p>
	<p><li> [Jan, 2023] <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10008078">GCNet</a> is accepted by TPAMI </p>
	<p><li> [Jul, 2022] <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9839579">PIRNet</a> is accepted by TNNLS </p>
	<p><li> [Jan, 2022] <a style="color: #447ec9" href="https://ieeexplore.ieee.org/document/9674867">SMIN</a> is accepted by TAC </p>
	<p><li> [Jan, 2021] <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9316758">CTNet</a> is accepted by TASLP </p>
  </div>  
  
  
 <!-- The Publications Section -->
  <div class="w3-container w3-padding-32"" id="publications">
    <h2> Research </h2>
	(* Equal contribution; † Corresponding author)
    <ol>
	  
    <p>
        <li><strong>OV-MER: Towards Open-Vocabulary Multimodal Emotion Recognition</strong>
        <br>
        <strong>Zheng Lian</strong>, Haiyang Sun, Licai Sun, Haoyu Chen, Lan Chen, Hao Gu, Zhuofan Wen, Shun Chen, Siyuan Zhang, Hailiang Yao, Bin Liu, Rui Liu, Shan Liang, Ya Li, Jiangyan Yi, Jianhua Tao
        <br>
        <em> Arxiv </em> 2025 | <a style="color: #447ec9" href="https://arxiv.org/abs/2410.01495">paper</a>
    </p>

	  <p>
      <li><strong>AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models</strong>
      <br>
	  <strong>Zheng Lian</strong>, Haoyu Chen, Lan Chen, Haiyang Sun, Licai Sun, Yong Ren, Zebang Cheng, Bin Liu, Rui Liu, Xiaojiang Peng, Jiangyan Yi, Jianhua Tao
      <br>
	  <em> Arxiv </em> 2025 | <a style="color: #447ec9" href="https://arxiv.org/abs/2501.16566">paper</a>
    </p>


    <p>
      <li><strong>QuMATL: Query-based Multi-annotator Tendency Learning</strong>
      <br>
      Liyun Zhang, <strong>Zheng Lian</strong>, Hong Liu, Takanori Takebe, Yuta Nakashima
      <br>
	  <em> Arxiv </em> 2025 | <a style="color: #447ec9" href="https://arxiv.org/abs/2503.15237">paper</a>
    </p>


    <p>
      <li><strong>EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models</strong>
      <br>
      He Hu, Yucheng Zhou, Lianzhong You, Hongbo Xu, Qianning Wang, <strong>Zheng Lian</strong>, Fei Richard Yu, Fei Ma, Laizhong Cui
      <br>
	  <em> Arxiv </em> 2025 | <a style="color: #447ec9" href="https://arxiv.org/abs/2502.04424">paper</a>
    </p>

    <p>
      <li><strong>P2Mark: Plug-and-play Parameter-intrinsic Watermarking for Neural Speech Generation</strong>
      <br>
      Yong Ren, Jiangyan Yi, Tao Wang, Jianhua Tao, Zhengqi Wen, Chenxing Li, <strong>Zheng Lian</strong>, Ruibo Fu, Ye Bai, Xiaohui Zhang
      <br>
	  <em> Arxiv </em> 2025 | <a style="color: #447ec9" href="https://arxiv.org/abs/2504.05197">paper</a>
    </p>

    <p>
      <li><strong>MEIJU-The 1st Multimodal Emotion and Intent Joint Understanding Challenge</strong>
      <br>
      Rui Liu, Xiaofen Xing, <strong>Zheng Lian</strong>, Haizhou Li, Björn W Schuller, Haolin Zuo
      <br>
	  <em> ICASSP  </em> 2025 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10890352/">paper</a>
    </p>

    <p>
      <li><strong>Adversarial Training and Gradient Optimization for Partially Deepfake Audio Localization</strong>
      <br>
      Siding Zeng, Jiangyan Yi, Jianhua Tao, Jiayi He, <strong>Zheng Lian</strong>, Shan Liang, Chuyuan Zhang, Yujie Chen, Xiaohui Zhang
      <br>
	  <em> ICASSP  </em> 2025 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10890470">paper</a>
    </p>

	  <p>
      <li><strong>Explainable Multimodal Emotion Recognition</strong>
      <br>
	  <strong>Zheng Lian</strong>, Haiyang Sun, Licai Sun, Hao Gu, Zhuofan Wen, Siyuan Zhang, etc.
      <br>
	  <em> Arxiv </em> 2024 | <a style="color: #447ec9" href="https://arxiv.org/abs/2306.15401">paper</a>
      </p>
	  
	  <p>
      <li><strong>MER 2024: Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition</strong>
      <br>
	  <strong>Zheng Lian</strong>, Haiyang Sun, Licai Sun, Zhuofan Wen, Siyuan Zhang, Shun Chen, Hao Gu, etc.
      <br>
	  <em> Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing </em> 2024 | <a style="color: #447ec9" href="https://dl.acm.org/doi/abs/10.1145/3689092.3689959">paper</a>
      </p>
	  
	  <p>
      <li><strong>GPT-4V with Emotion: A Zero-shot Benchmark for Generalized Emotion Recognition</strong>
      <br>
	  <strong>Zheng Lian</strong>, Licai Sun, Haiyang Sun, Kang Chen, Zhuofan Wen, Hao Gu, Bin Liu, Jianhua Tao
      <br>
	  <em> Information Fusion </em> 2024 | <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/pii/S1566253524001453">paper</a>
      </p>
    
      <p>
        <li><strong>Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning</strong>
        <br>
        Zebang Cheng, Zhi-Qi Cheng, Jun-Yan He, Jingdong Sun, Kai Wang, Yuxiang Lin, <strong>Zheng Lian</strong>, Xiaojiang Peng, Alexander Hauptmann
        <br>
      <em> NeurIPS </em> 2024 | <a style="color: #447ec9" href="https://arxiv.org/abs/2406.11161">paper</a>
        </p>
	  
	  <p>
      <li><strong>HiCMAE: Hierarchical Contrastive Masked Autoencoder for Self-Supervised Audio-Visual Emotion Recognition</strong>
      <br>
	  Licai Sun, <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao
      <br>
	  <em> Information Fusion </em> 2024 | <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/pii/S156625352400160X">paper</a>
      </p>
	  
	  <p>
      <li><strong>SVFAP: Self-supervised Video Facial Affect Perceiver</strong>
      <br>
	  Licai Sun, <strong>Zheng Lian</strong>, Kexin Wang, Yu He, Mingyu Xu, Haiyang Sun, Bin Liu, Jianhua Tao
      <br>
	  <em> IEEE Transactions on Affective Computing (TAC) </em> 2024 | <a style="color: #447ec9" href="https://arxiv.org/abs/2401.00416">paper</a>
      </p>
	  
	  
	  <p>
      <li><strong>Pseudo Labels Regularization for Imbalanced Partial-Label Learning</strong>
      <br>
	  Mingyu Xu, <strong>Zheng Lian</strong>, Bin Liu, Zerui Chen, Jianhua Tao
      <br>
      <em> ICASSP </em> 2024 |  <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10448034">paper</a>
      </p>
	  
	  <p>
      <li><strong>NLoPT: N-gram Enhanced Low-Rank Task Adaptive Pre-training for Efficient Language Model Adaption</strong>
      <br>
	  Hao Gu, Jiangyan Yi, <strong>Zheng Lian</strong>, Jianhua Tao, Xinrui Yan
      <br>
	  <em> LREC-COLING </em> 2024
      </p>
	  
	  <p>
      <li><strong>Contrastive Learning based Modality-Invariant Feature Acquisition for Robust Multimodal Emotion Recognition with Missing Modalities</strong>
      <br>
	  Rui Liu, Haolin Zuo, <strong>Zheng Lian</strong>, Björn W. Schuller, Haizhou Li
      <br>
	  <em> IEEE Transactions on Affective Computing (TAC) </em> 2024 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10474146">paper</a>
      </p>
    
    
      <p>
        <li><strong>MFSN: Multi-perspective Fusion Search Network For Pre-training Knowledge in Speech Emotion Recognition</strong>
        <br>
        
Haiyang Sun, Fulin Zhang, Yingying Gao, <strong>Zheng Lian</strong>, Shilei Zhang, Junlan Feng
        <br>
      <em> Interspeech </em> 2024 | <a style="color: #447ec9" href="https://arxiv.org/abs/2306.09361">paper</a>
        </p>
        
	  <p>
      <li><strong>MERBench: A Unified Evaluation Benchmark for Multimodal Emotion Recognition</strong>
      <br>
	  <strong>Zheng Lian</strong>, Licai Sun, Yong Ren, Hao Gu, Haiyang Sun, Lan Chen, Bin Liu, Jianhua Tao
      <br>
	  <em> Arxiv </em> 2023 | <a style="color: #447ec9" href="https://arxiv.org/pdf/2401.03429.pdf">paper</a>
      </p>
	  
	  
	  
	  <p>
      <li><strong>GCNet: Graph Completion Network for Incomplete Multimodal Learning in Conversation</strong>
      <br>
	  <strong>Zheng Lian</strong>, Lan Chen, Licai Sun, Bin Liu, Jianhua Tao
      <br>
      <em> Transactions on Pattern Analysis and Machine Intelligence (TPAMI) </em> 2023 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10008078">paper</a>
      </p>
	  
	  <p>
      <li><strong>MER 2023: Multi-label Learning, Modality Robustness, and Semi-Supervised Learning</strong>
      <br>
	  <strong>Zheng Lian</strong>, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, etc.
      <br>
      <em> ACM Multimedia (Organize Grand Challenge) </em> 2023 |  <a style="color: #447ec9" href="https://dl.acm.org/doi/pdf/10.1145/3581783.3612836">paper</a>
      </p>
	  
	  
	  <p>
      <li><strong>ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning</strong>
      <br>
	  Mingyu Xu*, <strong>Zheng Lian*</strong>, Lei Feng, Bin Liu, Jianhua Tao
      <br>
      <em> NeurIPS </em> 2023 |  <a style="color: #447ec9" href="https://openreview.net/forum?id=PYSfn5xXEe">paper</a>
      </p>
	  
	  <p>
      <li><strong>VRA: Variational Rectified Activation for Out-of-distribution Detection</strong>
      <br>
	  Mingyu Xu, <strong>Zheng Lian†</strong>, Bin Liu, Jianhua Tao
      <br>
      <em> NeurIPS </em> 2023 |  <a style="color: #447ec9" href="https://openreview.net/forum?id=d0VItRE2ZH">paper</a>
      </p>
	  
	  <p>
      <li><strong>MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic Facial Expression Recognition</strong>
      <br>
	  Licai Sun, <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao
      <br>
      <em> ACM Multimedia </em> 2023 |  <a style="color: #447ec9" href="https://dl.acm.org/doi/pdf/10.1145/3581783.3612365">paper</a>
      </p>
	  
	  <p>
      <li><strong>Efficient multimodal transformer with dual-level feature restoration for robust multimodal sentiment analysis</strong>
      <br>
	  Licai Sun, <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao
      <br>
      <em> IEEE Transactions on Affective Computing (TAC) </em> 2023 |  <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10122560">paper</a>
      </p>

	  <p>
      <li><strong>EmotionNAS: Two-stream Architecture Search for Speech Emotion Recognition</strong>
      <br>
      Haiyang Sun*, <strong>Zheng Lian*</strong>, Bin Liu, Ying Li, Licai Sun, Cong Cai, Jianhua Tao, Meng Wang, Yuan Cheng
      <br>
      <em>Interspeech </em> 2023 | <a style="color: #447ec9" href="https://arxiv.org/pdf/2203.13617.pdf">paper</a>
      </p>
	  
	  <p>
      <li><strong>PIRNet: Personality-enhanced Iterative Refinement Network for Emotion Recognition in Conversation</strong>
      <br>
	  <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao
      <br>
      <em> IEEE Transactions on Neural Networks and Learning Systems (TNNLS) </em> 2022 |  <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9839579">Early Access</a>
      </p>
	  
	  <p>
      <li><strong>SMIN: Semi-supervised Multi-modal Interaction Network for Conversational Emotion Recognition</strong>
      <br>
	  <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao
      <br>
      <em> IEEE Transactions on Affective Computing (TAC) </em> 2022 |  <a style="color: #447ec9" href="https://ieeexplore.ieee.org/document/9674867">paper</a>
      </p>
		
	  <p>
      <li><strong>DECN: Dialogical Emotion Correction Network for Conversational Emotion Recognition</strong>
      <br>
      <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao 
      <br>
      <em>Neurocomputing</em> 2021 | <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/pii/S0925231221007542">paper</a>
      </p>
	  
	  <p>
      <li><strong>CTNet: Conversational Transformer Network for Emotion Recognition</strong>
      <br>
      <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao 
      <br>
      <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)</em> 2021 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9316758">paper</a>
      </p>
	  
	  <p>
      <li><strong>Towards Fine-Grained Prosody Control for Voice Conversion</strong>
      <br>
      <strong>Zheng Lian</strong>, Rongxiu Zhong, Zhengqi Wen, Bin Liu, Jianhua Tao
      <br>
      <em>Proceedings of the 12th International Symposium on Chinese Spoken Language Processing (ISCSLP) </em> 2021 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9362110">paper</a>
      </p>
	  
	  
	  <p>
      <li><strong>Multimodal Cross-and Self-Attention Network for Speech Emotion Recognition</strong>
      <br>
	  Licai Sun, Bin Liu, Jianhua Tao, <strong>Zheng Lian</strong>
      <br>
      <em> ICASSP</em> 2021 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9414654">paper</a>
      </p>
	  
	  <p>
      <li><strong>Conversational Emotion Recognition Using Self-Attention Mechanisms and Graph Neural Networks</strong>
      <br>
      <strong>Zheng Lian</strong>, Jianhua Tao, Bin Liu, Jian Huang, Zhanlei Yang, Rongjun Li
      <br>
      <em>Interspeech </em> 2020 | <a style="color: #447ec9" href="http://www.interspeech2020.org/uploadfile/pdf/Wed-1-9-6.pdf">paper</a>
      </p>
	  
	  <p>
      <li><strong>Context-Dependent Domain Adversarial Neural Network for Multimodal Emotion Recognition</strong>
      <br>
      <strong>Zheng Lian</strong>, Jianhua Tao, Bin Liu, Jian Huang, Zhanlei Yang, Rongjun Li
      <br>
      <em>Interspeech </em> 2020 | <a style="color: #447ec9" href="http://www.interspeech2020.org/uploadfile/pdf/Mon-1-9-9.pdf">paper</a>
      </p>
	
	  
	  <p>
      <li><strong>ARVC: An Auto-Regressive Voice Conversion System Without Parallel Training Data</strong>
      <br>
      <strong>Zheng Lian</strong>, Zhengqi Wen, Xinyong Zhou, Songbai Pu, Shengkai Zhang, Jianhua Tao
      <br>
      <em>Interspeech </em> 2020 | <a style="color: #447ec9" href="http://www.interspeech2020.org/uploadfile/pdf/Thu-3-4-7.pdf">paper</a>
      </p>	  
	  
	  <p>
      <li><strong>Learning Utterance-level Representations with Label Smoothing for Speech Emotion Recognition</strong>
      <br>
	  Jian Huang, Jianhua Tao, Bin Liu, <strong>Zheng Lian</strong>
      <br>
      <em>Interspeech </em> 2020 | <a style="color: #447ec9" href="http://www.interspeech2020.org/uploadfile/pdf/Thu-2-2-1.pdf">paper</a>
      </p>	
	  
	  <p>
      <li><strong>Multimodal Transformer Fusion for Continuous Emotion Recognition</strong>
      <br>
	  Jian Huang, Jianhua Tao, Bin Liu, <strong>Zheng Lian</strong>, Mingyue Niu
      <br>
      <em>ICASSP </em> 2020 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9053762">paper</a>
      </p>	
	  
	  <p>
      <li><strong>Multimodal Spatiotemporal Representation for Automatic Depression Level Detection</strong>
      <br>
	  Mingyue Niu, Jianhua Tao, Bin Liu, Jian Huang, <strong>Zheng Lian</strong>
      <br>
      <em>IEEE Transactions on Affective Computing (TAC) </em> 2020 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9226102">paper</a>
      </p>	

	  <p>
      <li><strong>Unsupervised Representation Learning with Future Observation Prediction for Speech Emotion Recognition</strong>
      <br>
      <strong>Zheng Lian</strong>, Jianhua Tao, Bin Liu, Jian Huang
      <br>
      <em>Interspeech </em> 2019 | <a style="color: #447ec9" href="https://www.isca-speech.org/archive/pdfs/interspeech_2019/lian19b_interspeech.pdf">paper</a>
      </p>	  

	  <p>
      <li><strong>Conversational Emotion Analysis via Attention Mechanisms</strong>
      <br>
      <strong>Zheng Lian</strong>, Jianhua Tao, Bin Liu, Jian Huang
      <br>
      <em>Interspeech </em> 2019 | <a style="color: #447ec9" href="https://www.isca-archive.org/interspeech_2019/lian19_interspeech.pdf">paper</a>
      </p>	 
	  
	  
	  <p>
      <li><strong>Discriminative video representation with temporal order for micro-expression recognition</strong>
      <br>
	  Mingyue Niu, Jianhua Tao, Ya Li, Jian Huang, <strong>Zheng Lian</strong>
      <br>
      <em>ICASSP </em> 2019 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/document/8682295">paper</a>
      </p>	 
	  
	  <p>
      <li><strong>End-to-End Continuous Emotion Recognition from Video Using 3D ConvLSTM Networks</strong>
      <br>
	  Jian Huang, Ya Li, Jianhua Tao, <strong>Zheng Lian</strong>, Jiangyan Yi
      <br>
      <em>ICASSP </em> 2018 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/document/8461963">paper</a>
      </p>	
	  
	  <p>
      <li><strong>Speech Emotion Recognition from Variable-Length Inputs with Triplet Loss Function</strong>
      <br>
	  Jian Huang, Ya Li, Jianhua Tao, <strong>Zheng Lian</strong>
      <br>
      <em>Interspeech </em> 2018 | <a style="color: #447ec9" href="https://www.isca-archive.org/interspeech_2018/huang18b_interspeech.pdf">paper</a>
      </p>	
	  	
      </ol>
	  

    </p>
  </div>


	
  <!-- The Awards Section -->
  <div class="w3-container w3-padding-32" id="award">
    <h2>Awards</h2>
  <p><li> 2024, Winner in the MuSe Challenge@ACM MM</p>
	<p><li> 2023, Best Poster in the First CSIG Conference on Emotional Intelligence </p>
	<p><li> 2023, Winner in the MEGC Challenge@ACM MM</p>
	<p><li> 2023, Outstanding Reviewer of ICASSP</p>
	<p><li> 2022, Winner in the MuSe Challenge@ACM MM</p>
	<p><li> 2021, The First Prize for Technological Invention of Chinese Institute of Electronics</p>
	<!-- 中国电子学会技术发明一等奖 -->
	<p><li> 2021, Best Paper for National Conference on Man-Machine Speech Communication (NCMMSC)</p>
	<!-- 第十六届全国人机语音通讯学术会议最佳论文奖 -->
	<p><li> 2021, Winner in the MuSe Challenge@ACM MM</p>
	<p><li> 2021, Climbing First-class Scholarship of Institute of Automation, Chinese Academy of Sciences</p>
	<!-- 中国科学院自动化研究所攀登一等奖学金 -->
	<p><li> 2020, Winner in the MuSe Challenge@ACM MM</p>
	<p><li> 2020, Interspeech Travel Grand</p>
	<p><li> 2020, Winner in Voice Conversion Challenge@Interspeech</p>
    <p><li> 2019, 2nd Place in the AVEC Challenge@ACM MM</p>
    <p><li> 2018, 2nd Place in the AVEC Challenge@ACM MM</p>
	<!--<p><li> 2017, "Three Good Students" of University of Chinese Academy of Sciences</p>-->
  </div>  
  
  
  <!-- The Professionals Section -->
  <div class="w3-container w3-padding-32" id="professionals">
    <h2>Academic Services</h2>
  <p><li> Co-organize MER'25 Challenge and MRAC'25 Workshop@ACM MM, 2025 </p>
  <p><li> Co-organize MEIJU'25 Challenge@ICASSP, 2025 </p>
  <p><li> Co-organize MER'24 Challenge@IJCAI and MRAC'24 Workshop@ACM MM, 2024 </p>
	<p><li> Co-organize MER'23 Challenge@ACM MM and MRAC'23 Workshop@ACM MM, 2023 </p>
	<p><li> Executive Committee Member, Chinese Information Processing Society of China, Emotional Computing Committee（2024-） </p>
  <p><li> Executive Committee Member, Speech Dialogue and Auditory Professional Committee, China Computer Federation (CCF)（2024-） </p>
  <p><li> Committee Member, Chinese Society of Image and Graphics, Emotional Computing and Understanding Committee（2021-） </p>
	<p><li> Committee Member, Chinese Association for Artificial Intelligence, Emotional Intelligence Committee（2021-） </p>
  <p><li> Associate Editor: IEEE Transactions on Affective Computing </p>
  <p><li> Area Editor: Information Fusion </p>
  <p><li> Area Chair: ACM Multimedia 2025 </p>
  <p><li> Session Chair: ISCSLP 2024 </p>
	<p><li> Conference Program Committee: NeurIPS, ICLR, ICML, AAAI, ACL ARR, ACM Multimedia, ICASSP, Interspeech, ISCSLP, etc. </p>
  <p><li> Journal Reviewer: TPAMI, IJCV, TAC, TNNLS, TASLP, TOMM, TCSVT, TALLIP, IEEE Signal Processing Magazine, Pattern Recognition, etc </p>
  </div> 
  

  <!-- The Projects Section -->
  <div class="w3-container w3-padding-32"" id="projects">
    <h2> Projects </h2>
	
    <ol>
	
      <p><li>National Natural Science Foundation of China, Youth Science Fund Project, 2023/1~2025/12, Host ￥300,000</p>
	    <!-- <p><li><strong>国家自然科学基金，青年科学基金项目，面向交互场景的情感识别方法研究，62201572，2023/1/1~2025/12/31，30万元，在研，主持</strong></p>-->
      
      <p><li>Chinese Academy of Sciences, 2023/1~2023/12, Host ￥500,000 (Total ￥9,600,000)</p>
      <!-- <p><li><strong>中国科学院重点部署项目，c*作业人员心理状态评估技术</strong></p>-->
        
      <p><li>National Natural Science Foundation of China, 2023/1~2026/12, Host ￥60,000 (Total ￥560,000)</p>
	    <!-- <p><li><strong>国家自然科学基金面上项目，小样本下基于语音内容理解的青少年抑郁症早期识别方法研究</strong></p>-->

      <p><li>Outstanding Youth Fund of State Key Laboratory of Multimodal Artificial Intelligence Systems, 2024/12~2025/12, Host ￥100,000 </p>
      <!-- <p><li><strong>多模态人工智能系统全国重点实验室2024年优秀青年基金，大模型与知识驱动的鲁棒细微情感交互技术</strong></p>-->
      
    </p>
  </div>
  
  
  <!-- The Patents Section -->
  <div class="w3-container w3-padding-32"" id="patents">
    <h2> Patents </h2>
	
    <ol>
      
	  <p>
      <li><strong>Dialogue emotion correction method based on graph neural network (US Patent No. 12100418), 2024/09/24 </strong>  <a style="color: #447ec9" href="https://patentimages.storage.googleapis.com/42/0c/46/0f263026e451a7/US12100418.pdf">Link</a>
      <br>
	  Jianhua Tao, <strong>Zheng Lian</strong>, Bin Liu, Xuefei Liu
      <br>
      </p>
	  
	  <p>
      <li><strong>Automatic lie detection method and apparatus for interactive scenarios, device and medium (US Patent No. 11238289), 2022/02/01 </strong>  <a style="color: #447ec9" href="https://patentimages.storage.googleapis.com/5f/ef/96/54eda53532f316/US11238289.pdf">Link</a>
      <br>
	  Jianhua Tao, <strong>Zheng Lian</strong>, Bin Liu, Licai Sun
      <br>
      </p>
	  
	  <p>
      <li><strong>Multimodal dimensional emotion recognition method (US Patent No. 11281945), 2022/03/22 </strong>  <a style="color: #447ec9" href="https://patentimages.storage.googleapis.com/28/6b/b6/d5d3ebfbcbeea4/US11281945.pdf">Link</a>
      <br>
	  Jianhua Tao, Licai Sun, Bin Liu, <strong>Zheng Lian</strong>
      <br>
      </p>
	  
	  <p>
      <li><strong>Multi-modal lie detection method and apparatus, and device (US Patent No. 11244119), 2022/02/08 </strong>  <a style="color: #447ec9" href="https://patentimages.storage.googleapis.com/c2/a6/4f/26204db8af95c5/US11244119.pdf">Link</a>
      <br>
	  Jianhua Tao, Licai Sun, Bin Liu, <strong>Zheng Lian</strong>
      <br>
      </p>
	  
	  <p>
      <li><strong>Expression recognition method under natural scene (US Patent No. 11216652), 2022/01/04</strong>  <a style="color: #447ec9" href="https://patentimages.storage.googleapis.com/22/2c/bc/043136d205e9c5/US11216652.pdf">Link</a>
      <br>
	  Jianhua Tao, Mingyuan Xiao, Bin Liu, <strong>Zheng Lian</strong>
      <br>
      </p>
	  
    </p>
  </div>
  
  
  
  <div class="w3-light-grey w3-center w3-padding-24">
  <script type="text/javascript">
  var sc_project=12347113; 
  var sc_invisible=0; 
  var sc_security="21aca5d1"; 
  var sc_https=1; 
  var scJsHost = "https://";
  document.write("<sc"+"ript type='text/javascript' src='" + scJsHost+
  "statcounter.com/counter/counter.js'></"+"script>");
  <noscript>
    <div class="statcounter"><a title="Web Analytics Made Easy -
  StatCounter" href="https://statcounter.com/" target="_blank"><img
  class="statcounter" src="https://c.statcounter.com/12347113/0/21aca5d1/0/"
  alt="Web Analytics Made Easy - StatCounter"></a></div>
  </noscript>
  <!-- End of Statcounter Code -->

  </div>

  <!-- End page content -->
</div>

<script>
// Accordion 
function myAccFunc() {
  var x = document.getElementById("demoAcc");
  if (x.className.indexOf("w3-show") == -1) {
    x.className += " w3-show";
  } else {
    x.className = x.className.replace(" w3-show", "");
  }
}

// Click on the "Jeans" link on page load to open the accordion for demo purposes
document.getElementById("myBtn").click();


// Open and close sidebar
function w3_open() {
  document.getElementById("mySidebar").style.display = "block";
  document.getElementById("myOverlay").style.display = "block";
}
 
function w3_close() {
  document.getElementById("mySidebar").style.display = "none";
  document.getElementById("myOverlay").style.display = "none";
}
</script>

</body>
</html>
